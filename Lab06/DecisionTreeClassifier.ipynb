{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 机器学习实验报告\n",
    "## Lab6：Decision Tree Classifier\n",
    "- 姓名：周钰宸\n",
    "- 学号：2111408\n",
    "- 专业：信息安全"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e863dd5ed071b80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 实验要求\n",
    "作业的提交格式参考之前的说明，提交到18329300691@163.com\n",
    "### 1.1 基本要求\n",
    "a)\t基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "b)\t基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "### 1.2 中级要求\n",
    "a)  对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "b)\t对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "### 1.3高级要求\n",
    "使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f43d9ac0a0168132"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 实验原理\n",
    "### 2.1 决策树剪枝\n",
    "剪枝是决策树算法中一个重要的概念，用于减少过拟合的风险并提高模型的泛化能力。剪枝有多种方法，最常见的两种是预剪枝（pre-pruning）和后剪枝（post-pruning）。预剪枝是在决策树构造过程中进行剪枝，而后剪枝是在决策树构造完成后进行剪枝。\n",
    "### 2.1.1后剪枝（Post-Pruning）方法\n",
    "后剪枝涉及到在决策树完全生成后，通过移除某些子树或节点来简化决策树。一个常用的后剪枝技术是基于验证集的性能来决定是否剪除某个节点（将其替换为最常见的类）。这种方法称为减少错误剪枝（Reduced Error Pruning）。\n",
    "\n",
    "#### 2.1.1.1实现步骤：\n",
    "1. **分割数据**：将数据分为训练集、测试集和验证集。\n",
    "2. **构建完整的树**：使用训练集构建决策树，直到每个叶节点都纯净（或达到最小样本数）。\n",
    "3. **剪枝**：对于树中的每个非叶节点，尝试将其替换为叶节点，并检查这种替换是否会在验证集上改善预测准确率。如果是，则进行剪枝。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8b84bbcd91799fe"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3dbe60ebd7012e9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 基础要求\n",
    "a) 基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "b) 基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cce9131bf9a28a37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 导入所需要的包"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2d50214eb60af2d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- encoding: utf-8 -*-\n",
    "\"\"\"\n",
    "@Project ：Machine Learning \n",
    "@File    ：main.py\n",
    "@IDE     ：PyCharm \n",
    "@Author  ：ErwinZhou\n",
    "@Date    ：2023/12/10 17:16 \n",
    "\"\"\"\n",
    "\n",
    "# ---初级要求---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# ---中级要求---\n",
    "import math\n",
    "import copy\n",
    "# ---高级要求---\n",
    "# ---拓展要求---"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:28:40.632757800Z",
     "start_time": "2023-12-13T11:28:40.152350900Z"
    }
   },
   "id": "ed6a5a49e87714fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 导入数据集\n",
    "基础要求中所需要的数据集是Watermelon-train1.csv和Watermelon-test1.csv。不过这里一口气统一的对所有数据集进行读取并展示，方便之后的使用。**首先打印在对离散值进行编码前的数据集样式。**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c78becaa7f7047f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Before encoding------------------\n",
      "------------------df_train1------------------\n",
      "     编号  色泽  根蒂  敲声  纹理 好瓜\n",
      "0    1  青绿  蜷缩  浊响  清晰  是\n",
      "1    2  乌黑  蜷缩  沉闷  清晰  是\n",
      "2    3  乌黑  蜷缩  浊响  清晰  是\n",
      "3    4  青绿  蜷缩  沉闷  清晰  是\n",
      "4    5  浅白  蜷缩  浊响  清晰  是\n",
      "5    6  青绿  稍蜷  浊响  清晰  是\n",
      "6    7  乌黑  稍蜷  浊响  稍糊  是\n",
      "7    8  乌黑  稍蜷  浊响  清晰  是\n",
      "8    9  乌黑  稍蜷  沉闷  稍糊  否\n",
      "9   10  青绿  硬挺  清脆  清晰  否\n",
      "10  11  浅白  硬挺  清脆  模糊  否\n",
      "11  12  浅白  蜷缩  浊响  模糊  否\n",
      "12  13  青绿  稍蜷  浊响  稍糊  否\n",
      "13  14  浅白  稍蜷  沉闷  稍糊  否\n",
      "14  15  浅白  蜷缩  浊响  模糊  否\n",
      "15  16  青绿  蜷缩  沉闷  稍糊  否\n",
      "(16, 6)\n",
      "------------------df_train2------------------\n",
      "     编号  色泽  根蒂  敲声  纹理     密度 好瓜\n",
      "0    1  青绿  蜷缩  浊响  清晰  0.697  是\n",
      "1    2  乌黑  蜷缩  沉闷  清晰  0.774  是\n",
      "2    3  乌黑  蜷缩  浊响  清晰  0.634  是\n",
      "3    4  青绿  蜷缩  沉闷  清晰  0.608  是\n",
      "4    5  浅白  蜷缩  浊响  清晰  0.556  是\n",
      "5    6  青绿  稍蜷  浊响  清晰  0.403  是\n",
      "6    7  乌黑  稍蜷  浊响  稍糊  0.481  是\n",
      "7    8  乌黑  稍蜷  浊响  清晰  0.437  是\n",
      "8    9  乌黑  稍蜷  沉闷  稍糊  0.666  否\n",
      "9   10  青绿  硬挺  清脆  清晰  0.243  否\n",
      "10  11  浅白  硬挺  清脆  模糊  0.245  否\n",
      "11  12  浅白  蜷缩  浊响  模糊  0.343  否\n",
      "12  13  青绿  稍蜷  浊响  稍糊  0.639  否\n",
      "13  14  浅白  稍蜷  沉闷  稍糊  0.657  否\n",
      "14  15  乌黑  稍蜷  浊响  清晰  0.360  否\n",
      "15  16  浅白  蜷缩  浊响  模糊  0.593  否\n",
      "16  17  青绿  蜷缩  沉闷  稍糊  0.719  否\n",
      "(17, 7)\n",
      "------------------df_test1------------------\n",
      "    编号  色泽  根蒂  敲声  纹理 好瓜\n",
      "0   1  浅白  蜷缩  浊响  清晰  是\n",
      "1   2  乌黑  稍蜷  沉闷  清晰  是\n",
      "2   3  乌黑  蜷缩  沉闷  清晰  是\n",
      "3   4  青绿  蜷缩  沉闷  稍糊  是\n",
      "4   5  浅白  蜷缩  浊响  清晰  是\n",
      "5   6  青绿  稍蜷  浊响  清晰  否\n",
      "6   7  乌黑  稍蜷  浊响  稍糊  否\n",
      "7   8  青绿  稍蜷  浊响  模糊  否\n",
      "8   9  乌黑  稍蜷  沉闷  稍糊  否\n",
      "9  10  青绿  硬挺  清脆  模糊  否\n",
      "(10, 6)\n",
      "------------------df_test2------------------\n",
      "    编号  色泽  根蒂  敲声  纹理     密度 好瓜\n",
      "0   1  乌黑  稍蜷  浊响  清晰  0.403  是\n",
      "1   2  青绿  稍蜷  浊响  稍糊  0.481  是\n",
      "2   3  乌黑  稍蜷  浊响  清晰  0.337  是\n",
      "3   4  乌黑  稍蜷  沉闷  稍糊  0.666  否\n",
      "4   5  青绿  硬挺  清脆  清晰  0.243  否\n",
      "(5, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------Before encoding------------------\")\n",
    "df_train1 = pd.read_csv('data/Watermelon-train1.csv', encoding='gbk')\n",
    "df_train2 = pd.read_csv('data/Watermelon-train2.csv', encoding='gbk')\n",
    "df_test1 = pd.read_csv('data/Watermelon-test1.csv', encoding='gbk')\n",
    "df_test2 = pd.read_csv('data/Watermelon-test2.csv', encoding='gbk')\n",
    "\n",
    "print(\"------------------df_train1------------------\\n\", df_train1)\n",
    "print(df_train1.shape)\n",
    "print(\"------------------df_train2------------------\\n\", df_train2)\n",
    "print(df_train2.shape)\n",
    "print(\"------------------df_test1------------------\\n\", df_test1)\n",
    "print(df_test1.shape)\n",
    "print(\"------------------df_test2------------------\\n\", df_test2)\n",
    "print(df_test2.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:28:42.961874700Z",
     "start_time": "2023-12-13T11:28:42.919341900Z"
    }
   },
   "id": "9e210c3fef1cffb0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到以下特征：\n",
    "* 本次实验数据集为中文，因此需要设置编码为gbk。\n",
    "* train1和test1全部为离散值（正好适用于ID3）。而train2和test2的纹理列出现了连续值（正好适用于C4.5和CART）。同时为了后续方便处理，**对于就有离散值的train1和test1需要进行对应的编码。**\n",
    "* 本次实验的数据集第一列为编号列，对后续没有作用，需要进行删除；\n",
    "* 除此之外，**最重要的是：本次实验数据集数量级非常小，因此可以推测可能会对后续的决策树构造产生一定影响，理论上应该也不会出现过拟合的情况。**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e39c8d8f9120c2a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 数据预处理\n",
    "基于上述分析，首先我们要做的就是对数据集进行预处理，主要包括删除编号列和**对数据集进行编码**两项任务。其中第二项任务完成后应该返回一个字典类型的变量，能够方便后续进行对照分析实验结果使用。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ed87bccde3c1960"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------After encoding------------------\n",
      "------------------df_train1------------------\n",
      "     色泽  根蒂  敲声  纹理  好瓜\n",
      "0    0   0   0   0   0\n",
      "1    1   0   1   0   0\n",
      "2    1   0   0   0   0\n",
      "3    0   0   1   0   0\n",
      "4    2   0   0   0   0\n",
      "5    0   1   0   0   0\n",
      "6    1   1   0   1   0\n",
      "7    1   1   0   0   0\n",
      "8    1   1   1   1   1\n",
      "9    0   2   2   0   1\n",
      "10   2   2   2   2   1\n",
      "11   2   0   0   2   1\n",
      "12   0   1   0   1   1\n",
      "13   2   1   1   1   1\n",
      "14   2   0   0   2   1\n",
      "15   0   0   1   1   1\n",
      "{'色泽': Index(['青绿', '乌黑', '浅白'], dtype='object'), '根蒂': Index(['蜷缩', '稍蜷', '硬挺'], dtype='object'), '敲声': Index(['浊响', '沉闷', '清脆'], dtype='object'), '纹理': Index(['清晰', '稍糊', '模糊'], dtype='object'), '好瓜': Index(['是', '否'], dtype='object')}\n",
      "(16, 5)\n",
      "------------------df_test1------------------\n",
      "    色泽  根蒂  敲声  纹理  好瓜\n",
      "0   0   0   0   0   0\n",
      "1   1   1   1   0   0\n",
      "2   1   0   1   0   0\n",
      "3   2   0   1   1   0\n",
      "4   0   0   0   0   0\n",
      "5   2   1   0   0   1\n",
      "6   1   1   0   1   1\n",
      "7   2   1   0   2   1\n",
      "8   1   1   1   1   1\n",
      "9   2   2   2   2   1\n",
      "{'色泽': Index(['浅白', '乌黑', '青绿'], dtype='object'), '根蒂': Index(['蜷缩', '稍蜷', '硬挺'], dtype='object'), '敲声': Index(['浊响', '沉闷', '清脆'], dtype='object'), '纹理': Index(['清晰', '稍糊', '模糊'], dtype='object'), '好瓜': Index(['是', '否'], dtype='object')}\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "def encode_labels(df):\n",
    "    \"\"\"\n",
    "    @description :Encode labels for discrete features\n",
    "    @param df: dataframe\n",
    "    @return df: the same df but with encoding labels\n",
    "            mappings: dictionary of label and its encoding for looking up\n",
    "    \"\"\"\n",
    "    mappings = {}\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == object:  # string for object\n",
    "            df[column], unique_labels = pd.factorize(df[column])  # use factorize to encode\n",
    "            mappings[column] = unique_labels\n",
    "    return df, mappings\n",
    "\n",
    "\n",
    "print(\"------------------After encoding------------------\")\n",
    "df_train1, mappings_df_train1 = encode_labels(df_train1.drop(['编号'], axis=1))\n",
    "df_test1, mappings_df_test1 = encode_labels(df_test1.drop(['编号'], axis=1))\n",
    "\n",
    "print(\"------------------df_train1------------------\\n\", df_train1)\n",
    "print(mappings_df_train1)\n",
    "print(df_train1.shape)\n",
    "print(\"------------------df_test1------------------\\n\", df_test1)\n",
    "print(mappings_df_test1)\n",
    "print(df_test1.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:28:45.930355300Z",
     "start_time": "2023-12-13T11:28:45.894348100Z"
    }
   },
   "id": "839c9fbf6c704e47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 ID3决策树构造\n",
    "于是来到了最关键的一步，现在我会定义一个DecisionTree类，具体实现如下：\n",
    "1. 定义对应的方法，暂定方法只有一个method取值为ID3，未来会进一步拓展C4.5或者CART，**实现类似机制和策略的接口，方便使用；**\n",
    "2. **calculate_entropy函数**：由于目前只有离散值的ID3，只实现对应的计算离散值的熵值。概率直接数数就行了。。。；\n",
    "3. **calculate_information_gain**：调用calculate_entropy计算熵来计算基于某个特征的信息增益。返回的是feature对应特征的信息增益；\n",
    "4. **best_feature_to_split**：根据此时中还剩下的特征列calculate_information_gain的返回结果，并返回其中**最大值对应的特征，即所需要选择的特征；**\n",
    "5. **id3：实现对应的ID3算法，具体如下。**\n",
    "* 计算当前节点的父节点类别，这是通过找出当前数据集中最常见的类别来实现的\n",
    "* 用best_feature_to_split函数来找出信息增益最大的特征，这将是我们用来分割数据集的特征\n",
    "* 创建了一个新的字典来表示决策树，字典的键是最佳特征的名字，值是一个空字典，这个空字典稍后将被填充为子树\n",
    "* 从特征列表中移除了最佳特征，因为我们已经使用了这个特征进行分割，所以在子树中不再需要它\n",
    "* 最后对于最佳特征的每个唯一值，创建了一个子数据集，这个子数据集只包含最佳特征等于该值的样本。然后递归地调用id3函数在这个子数据集上构建子树，并将子树添加到决策树中\n",
    "* **递归截止条件一共有三条：**\n",
    "（1）当前结点所有样本都属于同⼀类别；\n",
    "（2）当前结点的所有属性值都相同，没有剩余属性可用来进一步划分样本；\n",
    "（3）达到叶子结点的最小样本数；\n",
    "**其中考虑到由于数据特征较小，因此树不会非常深，故没有设计达到最大树深这一停止条件。**\n",
    "6. **predict**：模型预测方法，对每一行应用调用predict_single。即对每一个实例预测。\n",
    "7. **predict_single**：遍历语法树找到对应的分类结果，只要达到叶节点即可。\n",
    "8. **calculate_accuracy**:计算准确率。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d7331770ae5730d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    @description :Decision Tree Classifier\n",
    "    @param method: For future extension to other methods, default for ID3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='ID3'):\n",
    "        \"\"\"\n",
    "        @description :Initialize the classifier\n",
    "        @param method: prediction method\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.tree = None\n",
    "\n",
    "    def calculate_entropy(self, y):\n",
    "        \"\"\"\n",
    "        @description: calculate entropy for\n",
    "        @param y:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # discrete values, count the bin is enough\n",
    "        class_counts = np.bincount(y)\n",
    "        probabilities = class_counts / len(y)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def calculate_information_gain(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        @description: calculate the information and store in a dictionary for future looking up\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @param feature:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        total_entropy = self.calculate_entropy(y)\n",
    "        values, counts = np.unique(X[feature], return_counts=True)\n",
    "        weighted_entropy = np.sum(\n",
    "            [(counts[i] / np.sum(counts)) * self.calculate_entropy(y[X[feature] == v]) for i, v in enumerate(values)])\n",
    "        # Calculate the difference to measure the information gain\n",
    "        information_gain = total_entropy - weighted_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def best_feature_to_split(self, X, y):\n",
    "        \"\"\"\n",
    "        @description :Find the best feature to split which has the biggest information gain\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @return: the name of the feature has the biggest information gain\n",
    "        \"\"\"\n",
    "        information_gains = {\n",
    "            feature: self.calculate_information_gain(X, y, feature)\n",
    "            for feature in X.columns}\n",
    "        # max on biggest value of information gain(not the key) in the dictionary\n",
    "        return max(information_gains, key=information_gains.get)\n",
    "\n",
    "    def id3(self, X, y, original_data, features, parent_node_class=None):\n",
    "        \"\"\"\n",
    "        @description :ID3 algorithm\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @param original_data:\n",
    "        @param features:\n",
    "        @param parent_node_class:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # Firstly, it can stop at three conditions(Ignore the situation that the tree reaches a certain depth):\n",
    "        if len(np.unique(y)) <= 1:\n",
    "            # 1. If there is only one class left, return it\n",
    "            return np.unique(y)[0]\n",
    "        elif len(y) == 0:\n",
    "            # 2. If there is no data left, return the most common class\n",
    "            return np.unique(original_data[\"好瓜\"])[\n",
    "                np.argmax(np.unique(original_data[\"好瓜\"],\n",
    "                                    return_counts=True)[1])]\n",
    "        elif len(features) == 0:\n",
    "            # 3. If there is no feature left, return the parent node class（which is the most common class）\n",
    "            return parent_node_class\n",
    "        else:\n",
    "            # Then, If none of the above conditions is satisfied, continue to split\n",
    "            parent_node_class = np.unique(y)[\n",
    "                np.argmax(np.unique(y, return_counts=True)[1])]\n",
    "            best_feature = self.best_feature_to_split(X, y)\n",
    "            tree = {best_feature: {}}\n",
    "            features = [i for i in features if i != best_feature]\n",
    "            for value in np.unique(X[best_feature]):\n",
    "                sub_data = X.where(X[best_feature] == value).dropna()\n",
    "                subtree = self.id3(sub_data, y[sub_data.index], original_data, features, parent_node_class)\n",
    "                tree[best_feature][value] = subtree\n",
    "            return tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        @description: train the model based on different given methods\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        features = X.columns.tolist()\n",
    "        if self.method == 'ID3':\n",
    "            self.tree = self.id3(X, y, pd.concat([X, y], axis=1), features)\n",
    "        else:\n",
    "            # to be continue......\n",
    "            raise Exception('Unknown method.')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        @description: using the re-trained model to predict the result\n",
    "        @param X:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # apply predict_single on every row of X\n",
    "        predictions = X.apply(self.predict_single, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        @description: This method is used to predict the class label for a single instance 'x'.\n",
    "        @param x:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # Traverses the decision tree on the values in 'x' until it reaches a leaf node.\n",
    "        tree = self.tree\n",
    "        while isinstance(tree, dict):\n",
    "            root = next(iter(tree))\n",
    "            tree = tree[root][x[root]]\n",
    "        # The class label of the leaf node is returned as the prediction for 'x'.\n",
    "        return tree\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        @description: calculate accuracy for true labels and predicted ones\n",
    "        @param y_true:\n",
    "        @param y_pred:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        return np.sum(y_true == y_pred) / len(y_true)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:28:50.095263500Z",
     "start_time": "2023-12-13T11:28:50.075228Z"
    }
   },
   "id": "aaffc44dc04e64df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 训练决策树构造，预测并输出分类精度\n",
    "在此处同时实现通过模型的训练构建决策树，并输出对应的决策树样貌。同时，通过模型的预测，输出对应的分类精度。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d306e254c43e9321"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ID3 Decision Tree------------------\n",
      "{'纹理': {0: {'根蒂': {0.0: 0, 1.0: 0, 2.0: 1}}, 1: {'色泽': {0.0: 1, 1.0: {'敲声': {0.0: 0, 1.0: 1}}, 2.0: 1}}, 2: 1}}\n",
      "Classify accuracy for Decision Tree Classifier ID3 is: 70.00%\n",
      "Classify error rate for Decision Tree Classifier ID3 is: 30.00%\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "X_train = df_train1.drop(['好瓜'], axis=1)\n",
    "y_train = df_train1['好瓜']\n",
    "X_test = df_test1.drop(['好瓜'], axis=1)\n",
    "y_test = df_test1['好瓜']\n",
    "\n",
    "# Create Decision Tree Model and train it\n",
    "Decision_Tree_Classifier_ID3 = DecisionTree(\"ID3\")\n",
    "Decision_Tree_Classifier_ID3.fit(X_train, y_train)\n",
    "\n",
    "# Predict and output the accuracy\n",
    "y_pred = Decision_Tree_Classifier_ID3.predict(X_test)\n",
    "accuracy = Decision_Tree_Classifier_ID3.calculate_accuracy(y_test, y_pred)\n",
    "# split the data\n",
    "X_train = df_train1.drop(['好瓜'], axis=1)\n",
    "y_train = df_train1['好瓜']\n",
    "X_test = df_test1.drop(['好瓜'], axis=1)\n",
    "y_test = df_test1['好瓜']\n",
    "\n",
    "# Create Decision Tree Model and train it\n",
    "Decision_Tree_Classifier_ID3 = DecisionTree(\"ID3\")\n",
    "Decision_Tree_Classifier_ID3.fit(X_train, y_train)\n",
    "\n",
    "# Predict and output the accuracy and error rate\n",
    "y_pred = Decision_Tree_Classifier_ID3.predict(X_test)\n",
    "accuracy = Decision_Tree_Classifier_ID3.calculate_accuracy(y_test, y_pred) \n",
    "error_rate = (1 - accuracy) * 100\n",
    "accuracy = accuracy * 100\n",
    "print(\"------------------ID3 Decision Tree------------------\")\n",
    "print(Decision_Tree_Classifier_ID3.tree)\n",
    "print(\"Classify accuracy for Decision Tree Classifier ID3 is: {:.2f}%\".format(accuracy))\n",
    "print(\"Classify error rate for Decision Tree Classifier ID3 is: {:.2f}%\".format(error_rate))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:28:52.858189900Z",
     "start_time": "2023-12-13T11:28:52.789308500Z"
    }
   },
   "id": "3f7ec539a1e93b05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.6 结果分析\n",
    "初级要求中，我们构造了对应的ID3决策树并且基于Watermelon-test1进行了预测。虽然出于时间原因，没有来得及对决策树进行一些可视化呈现，不过可以看到输出的决策树大概摸样。**结果发现分类精度还可以，达到了70%，错误率仅有30%。证明分类器实现较为成功。**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a37bf7b19d9fa4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 中级要求\n",
    "a)  对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "b)\t对测试集Watermelon-test2进行预测，输出分类精度；"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39836b6590c0f6c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 C4.5改进\n",
    "由于C4.5和之前构造的ID3决策树的区别主要在于C4.5可以处理连续型属性，因此我们需要对之前的代码进行一定的改进。**主要改进的地方有以下几个：**\n",
    "1. **连续特征的处理**：\n",
    "   - ID3算法仅能处理离散特征，而C4.5扩展了这一点，能够处理连续特征。\n",
    "   - `best_feature_to_split_c4_5`函数针对连续特征寻找最佳分割点，并计算基于这些分割点的信息增益。(这里由于其实现和ID3算法的`best_feature_to_split_id3`函数参数等都不相同，因此我暂时没有把二者合二为一。)\n",
    "\n",
    "2. **信息增益比**：\n",
    "   - C4.5使用信息增益比（Information Gain Ratio）而不是仅仅使用信息增益（Information Gain），这有助于减少对具有较多值的特征的偏好。\n",
    "   - `calculate_information_gain` 方法中，信息增益的计算被用于ID3，而对于C4.5，我在通过调用修改的`calculate_entropy`来计算信息增益比。\n",
    "\n",
    "3. **不同的树构建逻辑**：\n",
    "   - 在`c4_5`方法中，根据特征是否连续使用不同的逻辑来构建树。对于连续特征，根据计算出的最佳分割点将数据分成两部分，并对每部分递归地构建子树。\n",
    "\n",
    "4. **预测逻辑的改变**：\n",
    "   - 在`predict_single`方法中，我添加了处理连续特征的逻辑。这包括检查特征值是否小于或大于分割点，并相应地遍历决策树。\n",
    "\n",
    "5. **数据过滤方法**：\n",
    "   - 实现了`filter_data`和`filter_data_by_threshold`方法来处理离散和连续特征的数据过滤。这些方法在树的构建过程中用于根据特征值分割数据集。\n",
    "6. **其它**：\n",
    "   - 除此之外，还有一些小的改进，比如我在DecisionTree类多添加了`self.features`和`self.features_properties`，**用于拓展针对于不同离散或者连续类型值的处理。**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d437d8eef25ee9ad"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    @description :Decision Tree Classifier\n",
    "    @param method: For future extension to other methods, default for ID3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method, features, features_properties):\n",
    "        \"\"\"\n",
    "        @description :Initialize the classifier\n",
    "        @param method: prediction method\n",
    "        @param features: AKA titles for each column\n",
    "        @param features_properties: feature properties(0 for discrete, 1 for continuous)\n",
    "        \"\"\"\n",
    "        self.features_properties = features_properties\n",
    "        self.features = features\n",
    "        self.method = method\n",
    "        self.tree = None\n",
    "\n",
    "    def most_common(self, data):\n",
    "        \"\"\"\n",
    "        @description: calculate the most common label in the last column\n",
    "        @param data:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # Extract the labels from the last column of the data\n",
    "        labels = data.iloc[:, -1]\n",
    "        # Sort the labels by their frequency in descending order\n",
    "        sorted_labels = labels.value_counts(sort=True)\n",
    "        # Return the most common label\n",
    "        return sorted_labels.keys()[0]\n",
    "\n",
    "    def filter_data(self, data, target_value, target_index):\n",
    "        \"\"\"\n",
    "        @description: filter the data based on a target value and index\n",
    "        @param data:\n",
    "        @param target_value:\n",
    "        @param target_index:\n",
    "        @return: a new data with entries that match the target value, excluding the target itself from each entry.\n",
    "        \"\"\"\n",
    "        filtered_data = []\n",
    "        for data_entry in data:\n",
    "            if data_entry[target_index] == target_value:\n",
    "                filtered_entry = data_entry[:target_index] + data_entry[target_index + 1:]\n",
    "                filtered_data.append(filtered_entry)\n",
    "        return filtered_data\n",
    "\n",
    "    def filter_data_by_threshold(self, data, feature_index, threshold, option='S'):\n",
    "        \"\"\"\n",
    "        @description: filter the data based on a threshold and index\n",
    "                      If option is 'S', it returns entries with feature value less than the threshold.\n",
    "                      If option is not 'S', it returns entries with feature value greater than the threshold.\n",
    "        @param data:\n",
    "        @param feature_index:\n",
    "        @param threshold:\n",
    "        @param option:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # Initialize an empty list to store the filtered data\n",
    "        filtered_data = []\n",
    "        for data_entry in data:\n",
    "            # If the option is 'S', check if the feature value is less than the threshold\n",
    "            if option == 'S':\n",
    "                if float(data_entry[feature_index]) < threshold:\n",
    "                    # If the feature value is less than the threshold, add the entry to the filtered data\n",
    "                    filtered_data.append(data_entry)\n",
    "            else:\n",
    "                # If the option is not 'S', check if the feature value is greater than the threshold\n",
    "                if float(data_entry[feature_index]) > threshold:\n",
    "                    # If the feature value is greater than the threshold, add the entry to the filtered data\n",
    "                    filtered_data.append(data_entry)\n",
    "\n",
    "        return filtered_data\n",
    "\n",
    "    def calculate_entropy(self, data):\n",
    "        \"\"\"\n",
    "        @description: calculate entropy for\n",
    "        @param data:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        if self.method == 'ID3':\n",
    "            # discrete values, count the bin is enough\n",
    "            class_counts = np.bincount(data)\n",
    "            probabilities = class_counts / len(data)\n",
    "            entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "            return entropy\n",
    "        elif self.method == 'C4.5':\n",
    "            # Calculate the total number of entries in the data\n",
    "            total_entries = len(data)\n",
    "            # Initialize a dictionary to store the frequency of each label\n",
    "            label_frequencies = {}\n",
    "            # Iterate over each entry in the data\n",
    "            for data_entry in data:\n",
    "                # Get the label of the entry\n",
    "                data_label = data_entry[-1]\n",
    "                # If the label is not in the dictionary, initialize it with 0\n",
    "                if data_label not in label_frequencies:\n",
    "                    label_frequencies[data_label] = 0\n",
    "                # Increment the count of the label in the dictionary\n",
    "                label_frequencies[data_label] += 1\n",
    "            # Initialize entropy with 0\n",
    "            calculated_entropy = 0\n",
    "            # Calculate the entropy\n",
    "            for label_count in label_frequencies.values():\n",
    "                # Calculate the probability of the label\n",
    "                probability = label_count / total_entries\n",
    "                # Update the entropy\n",
    "                calculated_entropy -= probability * math.log2(probability)\n",
    "            # Return the calculated entropy\n",
    "            return calculated_entropy\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"No such method!\")\n",
    "\n",
    "    def calculate_information_gain(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        @description: calculate the information and store in a dictionary for future looking up\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @param feature:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        total_entropy = self.calculate_entropy(y)\n",
    "        values, counts = np.unique(X[feature], return_counts=True)\n",
    "        weighted_entropy = np.sum(\n",
    "            [(counts[i] / np.sum(counts)) * self.calculate_entropy(y[X[feature] == v]) for i, v in enumerate(values)])\n",
    "        # Calculate the difference to measure the information gain\n",
    "        information_gain = total_entropy - weighted_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def best_feature_to_split_id3(self, X, y):\n",
    "        \"\"\"\n",
    "        @description :Find the best feature to split which has the biggest information gain\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @return: the name of the feature has the biggest information gain\n",
    "        \"\"\"\n",
    "        information_gains = {\n",
    "            feature: self.calculate_information_gain(X, y, feature)\n",
    "            for feature in X.columns}\n",
    "        # max on biggest value of information gain(not the key) in the dictionary\n",
    "        return max(information_gains, key=information_gains.get)\n",
    "\n",
    "    def best_feature_to_split_c4_5(self, X, fes_prpers):\n",
    "        \"\"\"\n",
    "        This function chooses the best feature for splitting the dataset based on information gain.\n",
    "        It correctly handles both discrete and continuous features.\n",
    "        \"\"\"\n",
    "        num_of_features = len(fes_prpers)\n",
    "        base_entropy_value = self.calculate_entropy(X)\n",
    "        max_info_gain = 0.0\n",
    "        best_feature_index = -1\n",
    "        best_partition_val = None\n",
    "        for i in range(num_of_features):\n",
    "            feature_values = [example[i] for example in X]\n",
    "            unique_feature_values = set(feature_values)\n",
    "            entropy_value = 0.0\n",
    "            best_partition_val_i = None\n",
    "            if fes_prpers[i] == 0:  # Discrete feature\n",
    "                for value in unique_feature_values:\n",
    "                    filtered_data = self.filter_data(X, value, i)\n",
    "                    probability = len(filtered_data) / float(len(X))\n",
    "                    entropy_value += probability * self.calculate_entropy(filtered_data)\n",
    "            else:  # Continuous feature\n",
    "                sorted_unique_feature_values = list(unique_feature_values)\n",
    "                sorted_unique_feature_values.sort()\n",
    "                min_entropy_value = 10000\n",
    "                for j in range(len(sorted_unique_feature_values) - 1):\n",
    "                    partition_val = (float(sorted_unique_feature_values[j]) + float(\n",
    "                        sorted_unique_feature_values[j + 1])) / 2\n",
    "                    left_data = self.filter_data_by_threshold(X, i, partition_val, 'S')\n",
    "                    right_data = self.filter_data_by_threshold(X, i, partition_val, 'T')\n",
    "                    left_probability = len(left_data) / float(len(X))\n",
    "                    right_probability = len(right_data) / float(len(X))\n",
    "                    entropy_val = left_probability * self.calculate_entropy(left_data) + right_probability * self.calculate_entropy(\n",
    "                        right_data)\n",
    "                    if entropy_val < min_entropy_value:\n",
    "                        min_entropy_value = entropy_val\n",
    "                        best_partition_val_i = partition_val\n",
    "                entropy_value = min_entropy_value\n",
    "            information_gain = base_entropy_value - entropy_value\n",
    "            if information_gain > max_info_gain:\n",
    "                max_info_gain = information_gain\n",
    "                best_feature_index = i\n",
    "                best_partition_val = best_partition_val_i\n",
    "        return best_feature_index, best_partition_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def id3(self, X, y, original_data, features, parent_node_class=None):\n",
    "        \"\"\"\n",
    "        @description :ID3 algorithm\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @param original_data:\n",
    "        @param features:\n",
    "        @param parent_node_class:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # Firstly, it can stop at three conditions(Ignore the situation that the tree reaches a certain depth):\n",
    "        if len(np.unique(y)) <= 1:\n",
    "            # 1. If there is only one class left, return it\n",
    "            return np.unique(y)[0]\n",
    "        elif len(y) == 0:\n",
    "            # 2. If there is no data left, return the most common class\n",
    "            return np.unique(original_data[\"好瓜\"])[\n",
    "                np.argmax(np.unique(original_data[\"好瓜\"],\n",
    "                                    return_counts=True)[1])]\n",
    "        elif len(features) == 0:\n",
    "            # 3. If there is no feature left, return the parent node class（which is the most common class）\n",
    "            return parent_node_class\n",
    "        else:\n",
    "            # Then, If none of the above conditions is satisfied, continue to split\n",
    "            parent_node_class = np.unique(y)[\n",
    "                np.argmax(np.unique(y, return_counts=True)[1])]\n",
    "            best_feature = self.best_feature_to_split_id3(X, y)\n",
    "            tree = {best_feature: {}}\n",
    "            features = [i for i in features if i != best_feature]\n",
    "            for value in np.unique(X[best_feature]):\n",
    "                sub_data = X.where(X[best_feature] == value).dropna()\n",
    "                subtree = self.id3(sub_data, y[sub_data.index], original_data, features, parent_node_class)\n",
    "                tree[best_feature][value] = subtree\n",
    "            return tree\n",
    "\n",
    "    def c4_5(self, X, fes, fe_propers):\n",
    "        \"\"\"\n",
    "        @description: C4.5 algorithm\n",
    "        @param X:\n",
    "        @param fes:\n",
    "        @param fe_propers:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # Get the class labels for each data entry\n",
    "        classList = [example[-1] for example in X]\n",
    "        # If all class labels are the same, return the class label\n",
    "        if classList.count(classList[0]) == len(classList):\n",
    "            return classList[0]\n",
    "        # If there is only one feature left, return the most common class label\n",
    "        if len(X[0]) == 1:\n",
    "            return self.most_common(classList)\n",
    "\n",
    "        # Choose the best feature and partition value for splitting\n",
    "        bestFeature, bestPartitionValue = self.best_feature_to_split_c4_5(X, fe_propers)\n",
    "\n",
    "        # If no best feature is found, return the most common class label\n",
    "        if bestFeature == -1:\n",
    "            return self.most_common(classList)\n",
    "\n",
    "        # If the best feature is discrete\n",
    "        if fe_propers[bestFeature] == 0:\n",
    "            bestFeatureLabel = fes[bestFeature]\n",
    "            Tree = {bestFeatureLabel: {}}\n",
    "            newLabels = copy.copy(fes)\n",
    "            newLabelProperties = copy.copy(fe_propers)\n",
    "            del (newLabels[bestFeature])\n",
    "            del (newLabelProperties[bestFeature])\n",
    "            featureValues = [example[bestFeature] for example in X]\n",
    "            uniqueValues = set(featureValues)\n",
    "            for value in uniqueValues:\n",
    "                subLabels = newLabels[:]\n",
    "                subLabelProperties = newLabelProperties[:]\n",
    "                Tree[bestFeatureLabel][value] = self.c4_5(self.filter_data(X, value, bestFeature), subLabels,\n",
    "                                                          subLabelProperties)\n",
    "        else:  # If the best feature is continuous\n",
    "            bestFeatureLabel = fes[bestFeature] + '<' + str(bestPartitionValue)\n",
    "            Tree = {bestFeatureLabel: {}}\n",
    "            subLabels = fes[:]\n",
    "            subLabelProperties = fe_propers[:]\n",
    "            # Build the left subtree\n",
    "            leftValue = '是'\n",
    "            Tree[bestFeatureLabel][leftValue] = self.c4_5(\n",
    "                self.filter_data_by_threshold(X, bestFeature, bestPartitionValue, 'S'), subLabels,\n",
    "                subLabelProperties)\n",
    "            # Build the right subtree\n",
    "            rightValue = '否'\n",
    "            Tree[bestFeatureLabel][rightValue] = self.c4_5(\n",
    "                self.filter_data_by_threshold(X, bestFeature, bestPartitionValue, 'T'), subLabels,\n",
    "                subLabelProperties)\n",
    "        return Tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        @description: train the model based on different given methods\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.method == 'ID3':\n",
    "            features = X.columns.tolist()\n",
    "            self.tree = self.id3(X, y, pd.concat([X, y], axis=1), features)\n",
    "        elif self.method == 'C4.5':\n",
    "            self.tree = self.c4_5(X, self.features, self.features_properties)\n",
    "        else:\n",
    "            raise Exception(\"No such method!\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        @description: using the re-trained model to predict the result\n",
    "        @param X:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        if self.method == 'ID3':\n",
    "            # apply predict_single on every row of X\n",
    "            predictions = X.apply(self.predict_single, axis=1)\n",
    "            return predictions\n",
    "        elif self.method == 'C4.5':\n",
    "            predictions = []\n",
    "            for x in X:\n",
    "                predictions.append(self.predict_single(self.tree, self.features, self.features_properties, x))\n",
    "        else:\n",
    "            raise Exception(\"No such method!\")\n",
    "\n",
    "        return predictions\n",
    "    def predict_single(self, DecisionTree, fes, fe_propers, test_data):\n",
    "        \"\"\"\n",
    "        @description: using the re-trained model to predict the result\n",
    "        @param DecisionTree: the whole decision tree or subtree\n",
    "        @param fes: feature labels\n",
    "        @param fe_propers: feature properties\n",
    "        @param test_data: the test data\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        if self.method == 'ID3':\n",
    "            # Traverses the decision tree on the values in 'x' until it reaches a leaf node.\n",
    "            tree = DecisionTree\n",
    "            while isinstance(tree, dict):\n",
    "                root = next(iter(tree))\n",
    "                tree = tree[root][test_data[root]]\n",
    "            # The class label of the leaf node is returned as the prediction for 'x'.\n",
    "            return tree\n",
    "        elif self.method == 'C4.5':\n",
    "            # Get the first key of the input tree\n",
    "            firstKey = list(DecisionTree.keys())[0]\n",
    "            featureName = firstKey\n",
    "            # Check if the feature is continuous\n",
    "            splitIndex = str(firstKey).find('<')\n",
    "            if splitIndex > -1:\n",
    "                featureName = str(firstKey)[:splitIndex]\n",
    "            # Get the subtree corresponding to the first key\n",
    "            subTree = DecisionTree[firstKey]\n",
    "            # Get the index of the feature in the feature labels list\n",
    "            featureIndex = fes.index(featureName)\n",
    "            predictedLabel = None\n",
    "            for key in subTree.keys():\n",
    "                if fe_propers[featureIndex] == 0:\n",
    "                    if test_data[featureIndex] == key:\n",
    "                        if type(subTree[key]).__name__ == 'dict':\n",
    "                            predictedLabel = self.predict_single(subTree[key], fes, fe_propers, test_data)\n",
    "                        else:\n",
    "                            predictedLabel = subTree[key]\n",
    "                else:\n",
    "                    partitionValue = float(str(firstKey)[splitIndex + 1:])\n",
    "                    if test_data[featureIndex] < partitionValue:\n",
    "                        if type(subTree['是']).__name__ == 'dict':\n",
    "                            predictedLabel = self.predict_single(subTree['是'], fes, fe_propers, test_data)\n",
    "                        else:\n",
    "                            predictedLabel = subTree['是']\n",
    "                    else:\n",
    "                        if type(subTree['否']).__name__ == 'dict':\n",
    "                            predictedLabel = self.predict_single(subTree['否'], fes, fe_propers, test_data)\n",
    "                        else:\n",
    "                            predictedLabel = subTree['否']\n",
    "\n",
    "            return predictedLabel\n",
    "        else:\n",
    "            raise Exception(\"No such method!\")\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        @description: calculate accuracy for true labels and predicted ones\n",
    "        @param y_true:\n",
    "        @param y_pred:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        return np.sum(y_true == y_pred) / len(y_true)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:28:56.301807900Z",
     "start_time": "2023-12-13T11:28:56.256374600Z"
    }
   },
   "id": "250fa2b39c7e3ad3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 数据预处理与决策树构建\n",
    "在重新改进了我们的决策树`DecisionTree`后，它如今**可以根据不同的模型给定方法适用于不同的类型数据集了。**不过在构建决策树之前，这次我需要传入记录特征类型的`features_properties`，**用于拓展针对于不同离散或者连续类型值的处理。** \n",
    "\n",
    "同时也要对数据进行一些处理，而不是像之前的可以简单的进行编码即可。**在处理完后，只需要传进参数`C4.5`，`features`和`features_properties`然后训练即可。**\n",
    "\n",
    "最后构建完成的决策树保留在`Decision_Tree_Classifier_C4_5.tree`中。输出出来结果如下。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88be4d33f549594a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------C4.5 Decision Tree------------------\n",
      "{'纹理': {'模糊': '否', '清晰': {'密度<0.3815': {'是': '否', '否': '是'}}, '稍糊': {'密度<0.56': {'是': '是', '否': '否'}}}}\n"
     ]
    }
   ],
   "source": [
    "train2 = np.array(df_train2).tolist()\n",
    "new_train2 = []\n",
    "for i in train2:\n",
    "    new_train2.append(i[1:])\n",
    "train2 = new_train2\n",
    "\n",
    "features = ['色泽', '根蒂', '敲声', '纹理', '密度']\n",
    "# 0 for discrete, 1 for continuous\n",
    "feature_properties = [0, 0, 0, 0, 1]\n",
    "\n",
    "# Create Decision Tree Model and train it\n",
    "Decision_Tree_Classifier_C4_5 = DecisionTree(\"C4.5\", features, feature_properties)\n",
    "\n",
    "Decision_Tree_Classifier_C4_5.fit(train2, df_train2['好瓜'])\n",
    "\n",
    "print(\"------------------C4.5 Decision Tree------------------\")\n",
    "print(Decision_Tree_Classifier_C4_5.tree)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:28:59.172348800Z",
     "start_time": "2023-12-13T11:28:59.152347600Z"
    }
   },
   "id": "31826b1632aec5ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 预测并输出分类精度\n",
    "现在我们只需对test2进行相同的数据处理后，调用`Decision_Tree_Classifier_C4_5.predict`方法即可。**最后输出了分类的精度结果如下：**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5198b9ec5c63e72f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify accuracy for Decision Tree Classifier C4.5 is: 80.00%\n",
      "Classify error rate for Decision Tree Classifier C4.5 is: 20.00%\n"
     ]
    }
   ],
   "source": [
    "test2 = np.array(df_test2).tolist()\n",
    "new_test2 = []\n",
    "for i in test2:\n",
    "    new_test2.append(i[1:])\n",
    "test2 = new_test2\n",
    "\n",
    "\n",
    "# Predict and output the accuracy\n",
    "y_pred = Decision_Tree_Classifier_C4_5.predict(test2)\n",
    "accuracy = Decision_Tree_Classifier_C4_5.calculate_accuracy(df_test2['好瓜'], y_pred)\n",
    "error_rate = (1 - accuracy) * 100\n",
    "accuracy = accuracy * 100\n",
    "print(\"Classify accuracy for Decision Tree Classifier C4.5 is: {:.2f}%\".format(accuracy))\n",
    "print(\"Classify error rate for Decision Tree Classifier C4.5 is: {:.2f}%\".format(error_rate))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:29:01.619679Z",
     "start_time": "2023-12-13T11:29:01.572225500Z"
    }
   },
   "id": "a91b77906ceda03c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 结果分析\n",
    "中级要求中，我们构造了对应的C4.5决策树并且基于Watermelon-test2进行了预测。**结果发现分类精度也还可以，达到了80%，错误率仅有20%。证明C4.5分类器实现较为成功。**\n",
    "\n",
    "同时其分类效果也优于ID3决策树，这也是由于C4.5能够处理连续型属性的优势所在。~~(虽然它们用的根本不是一个数据集吧）~~"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b29efdeae32f046"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 高级要求\n",
    "使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92d751df4e10db18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 后剪枝方法实现\n",
    "首先我们之前已经介绍过了后剪枝方法，现在尝试先实现基本的DecisionTree的后剪枝方法，然后再将其封装进我的DecisionTree类中。**实现的具体思路如下：主要通过验证数据集来剪枝以提高模型的泛化性能。**\n",
    "\n",
    "1. **递归遍历树：** 代码首先检查当前节点是否是叶子节点（不是一个字典类型的节点）。如果是叶子节点，表示已经到达决策树的底部，不需要再剪枝，直接返回该叶子节点。\n",
    "\n",
    "2. **递归遍历子节点：** 如果当前节点不是叶子节点，代码会进一步递归遍历当前节点的子节点（字典中的键值对）。这是一个深度优先搜索的过程，将对每个子节点执行相同的操作。\n",
    "\n",
    "3. **计算原始精度和剪枝精度：** 对于每个子节点，代码会计算在验证数据集上的原始精度（original_accuracy），这是在未剪枝时的精度。然后，它使用之前的 `most_common` 计算该子节点的最常见的类别值，并用这个值来替代子节点的决策，从而进行剪枝。接着，它再次计算在验证数据集上的剪枝后的精度（pruned_accuracy）。\n",
    "\n",
    "4. **判断是否剪枝：** 最后，代码会比较剪枝后的精度（pruned_accuracy）与原始精度（original_accuracy）。如果剪枝后的精度大于或等于原始精度，说明剪枝后的决策树更具泛化性能，可以进行剪枝操作，将当前节点替换为叶子节点，并返回剪枝后的节点。否则，保持原样返回当前节点，不进行剪枝。\n",
    "\n",
    "这个后剪枝过程通过递归地检查每个子节点，并根据验证数据集的性能来决定是否剪枝，有助于减小决策树的复杂度，提高模型的泛化性能。**主要用于防止过拟合。**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f1ef49e2ba7bf87"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def post_prune(self, node, validation_data):\n",
    "    \"\"\"\n",
    "    @description: Post-pruning method for the decision tree\n",
    "    @param node: current node\n",
    "    @param validation_data: validation data set\n",
    "    @return: pruned decision tree\n",
    "    \"\"\"\n",
    "    # Check if the current node is a leaf node\n",
    "    if not isinstance(node, dict):\n",
    "        return node\n",
    "\n",
    "    # Recursively prune child nodes\n",
    "    for key in node.keys():\n",
    "        node[key] = self.post_prune(node[key], validation_data)\n",
    "\n",
    "    # Check if the current node can be pruned\n",
    "    original_accuracy = self.calculate_accuracy(validation_data['label'], self.predict(validation_data))\n",
    "    pruned_node = self.most_common(validation_data['label'])\n",
    "    pruned_accuracy = self.calculate_accuracy(validation_data['label'], self.predict(validation_data, pruned_node))\n",
    "\n",
    "    # If the accuracy improves after pruning, prune the node\n",
    "    if pruned_accuracy >= original_accuracy:\n",
    "        return pruned_node\n",
    "    else:\n",
    "        return node"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:29:05.484385500Z",
     "start_time": "2023-12-13T11:29:05.461222700Z"
    }
   },
   "id": "cfb6ea9ad31d8c14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 决策树后剪枝封装\n",
    "然后我们只需要将其加入到`fit方法`中，再ID3和C4.5正常的决策树训练构建结束后，直接开始后剪枝构造真正树结果。最后再稍微修改下predict和predict_single的一些细节，目的就是实现对部分子树的预测。**封装后的完整DecisionTree类如下：**\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48457a4f6c10025"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    @description :Decision Tree Classifier\n",
    "    @param method: For future extension to other methods, default for ID3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method, features, features_properties, puring=False):\n",
    "        \"\"\"\n",
    "        @description :Initialize the classifier\n",
    "        @param method: prediction method\n",
    "        @param features: AKA titles for each column\n",
    "        @param features_properties: feature properties(0 for discrete, 1 for continuous)\n",
    "        @param puring: whether to use puring and which method to implement puring\n",
    "        \"\"\"\n",
    "        self.puring = puring\n",
    "        self.features_properties = features_properties\n",
    "        self.features = features\n",
    "        self.method = method\n",
    "        self.tree = None\n",
    "\n",
    "    def most_common(self, data):\n",
    "        \"\"\"\n",
    "        @description: calculate the most common label in the last column\n",
    "        @param data:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # Extract the labels from the last column of the data\n",
    "        labels = data.iloc[:, -1]\n",
    "        # Sort the labels by their frequency in descending order\n",
    "        sorted_labels = labels.value_counts(sort=True)\n",
    "        # Return the most common label\n",
    "        return sorted_labels.keys()[0]\n",
    "\n",
    "    def filter_data(self, data, target_value, target_index):\n",
    "        \"\"\"\n",
    "        @description: filter the data based on a target value and index\n",
    "        @param data:\n",
    "        @param target_value:\n",
    "        @param target_index:\n",
    "        @return: a new data with entries that match the target value, excluding the target itself from each entry.\n",
    "        \"\"\"\n",
    "        filtered_data = []\n",
    "        for data_entry in data:\n",
    "            if data_entry[target_index] == target_value:\n",
    "                filtered_entry = data_entry[:target_index] + data_entry[target_index + 1:]\n",
    "                filtered_data.append(filtered_entry)\n",
    "        return filtered_data\n",
    "\n",
    "    def filter_data_by_threshold(self, data, feature_index, threshold, option='S'):\n",
    "        \"\"\"\n",
    "        @description: filter the data based on a threshold and index\n",
    "                      If option is 'S', it returns entries with feature value less than the threshold.\n",
    "                      If option is not 'S', it returns entries with feature value greater than the threshold.\n",
    "        @param data:\n",
    "        @param feature_index:\n",
    "        @param threshold:\n",
    "        @param option:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # Initialize an empty list to store the filtered data\n",
    "        filtered_data = []\n",
    "        for data_entry in data:\n",
    "            # If the option is 'S', check if the feature value is less than the threshold\n",
    "            if option == 'S':\n",
    "                if float(data_entry[feature_index]) < threshold:\n",
    "                    # If the feature value is less than the threshold, add the entry to the filtered data\n",
    "                    filtered_data.append(data_entry)\n",
    "            else:\n",
    "                # If the option is not 'S', check if the feature value is greater than the threshold\n",
    "                if float(data_entry[feature_index]) > threshold:\n",
    "                    # If the feature value is greater than the threshold, add the entry to the filtered data\n",
    "                    filtered_data.append(data_entry)\n",
    "\n",
    "        return filtered_data\n",
    "\n",
    "    def calculate_entropy(self, data):\n",
    "        \"\"\"\n",
    "        @description: calculate entropy for\n",
    "        @param data:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        if self.method == 'ID3':\n",
    "            # discrete values, count the bin is enough\n",
    "            class_counts = np.bincount(data)\n",
    "            probabilities = class_counts / len(data)\n",
    "            entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "            return entropy\n",
    "        elif self.method == 'C4.5':\n",
    "            # Calculate the total number of entries in the data\n",
    "            total_entries = len(data)\n",
    "            # Initialize a dictionary to store the frequency of each label\n",
    "            label_frequencies = {}\n",
    "            # Iterate over each entry in the data\n",
    "            for data_entry in data:\n",
    "                # Get the label of the entry\n",
    "                data_label = data_entry[-1]\n",
    "                # If the label is not in the dictionary, initialize it with 0\n",
    "                if data_label not in label_frequencies:\n",
    "                    label_frequencies[data_label] = 0\n",
    "                # Increment the count of the label in the dictionary\n",
    "                label_frequencies[data_label] += 1\n",
    "            # Initialize entropy with 0\n",
    "            calculated_entropy = 0\n",
    "            # Calculate the entropy\n",
    "            for label_count in label_frequencies.values():\n",
    "                # Calculate the probability of the label\n",
    "                probability = label_count / total_entries\n",
    "                # Update the entropy\n",
    "                calculated_entropy -= probability * math.log2(probability)\n",
    "            # Return the calculated entropy\n",
    "            return calculated_entropy\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"No such method!\")\n",
    "\n",
    "    def calculate_information_gain(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        @description: calculate the information and store in a dictionary for future looking up\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @param feature:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        total_entropy = self.calculate_entropy(y)\n",
    "        values, counts = np.unique(X[feature], return_counts=True)\n",
    "        weighted_entropy = np.sum(\n",
    "            [(counts[i] / np.sum(counts)) * self.calculate_entropy(y[X[feature] == v]) for i, v in enumerate(values)])\n",
    "        # Calculate the difference to measure the information gain\n",
    "        information_gain = total_entropy - weighted_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def best_feature_to_split_id3(self, X, y):\n",
    "        \"\"\"\n",
    "        @description :Find the best feature to split which has the biggest information gain\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @return: the name of the feature has the biggest information gain\n",
    "        \"\"\"\n",
    "        information_gains = {\n",
    "            feature: self.calculate_information_gain(X, y, feature)\n",
    "            for feature in X.columns}\n",
    "        # max on biggest value of information gain(not the key) in the dictionary\n",
    "        return max(information_gains, key=information_gains.get)\n",
    "\n",
    "    def best_feature_to_split_c4_5(self, X, fes_prpers):\n",
    "        \"\"\"\n",
    "        This function chooses the best feature for splitting the dataset based on information gain.\n",
    "        It correctly handles both discrete and continuous features.\n",
    "        \"\"\"\n",
    "        num_of_features = len(fes_prpers)\n",
    "        base_entropy_value = self.calculate_entropy(X)\n",
    "        max_info_gain = 0.0\n",
    "        best_feature_index = -1\n",
    "        best_partition_val = None\n",
    "        for i in range(num_of_features):\n",
    "            feature_values = [example[i] for example in X]\n",
    "            unique_feature_values = set(feature_values)\n",
    "            entropy_value = 0.0\n",
    "            best_partition_val_i = None\n",
    "            if fes_prpers[i] == 0:  # Discrete feature\n",
    "                for value in unique_feature_values:\n",
    "                    filtered_data = self.filter_data(X, value, i)\n",
    "                    probability = len(filtered_data) / float(len(X))\n",
    "                    entropy_value += probability * self.calculate_entropy(filtered_data)\n",
    "            else:  # Continuous feature\n",
    "                sorted_unique_feature_values = list(unique_feature_values)\n",
    "                sorted_unique_feature_values.sort()\n",
    "                min_entropy_value = 10000\n",
    "                for j in range(len(sorted_unique_feature_values) - 1):\n",
    "                    partition_val = (float(sorted_unique_feature_values[j]) + float(\n",
    "                        sorted_unique_feature_values[j + 1])) / 2\n",
    "                    left_data = self.filter_data_by_threshold(X, i, partition_val, 'S')\n",
    "                    right_data = self.filter_data_by_threshold(X, i, partition_val, 'T')\n",
    "                    left_probability = len(left_data) / float(len(X))\n",
    "                    right_probability = len(right_data) / float(len(X))\n",
    "                    entropy_val = left_probability * self.calculate_entropy(\n",
    "                        left_data) + right_probability * self.calculate_entropy(\n",
    "                        right_data)\n",
    "                    if entropy_val < min_entropy_value:\n",
    "                        min_entropy_value = entropy_val\n",
    "                        best_partition_val_i = partition_val\n",
    "                entropy_value = min_entropy_value\n",
    "            information_gain = base_entropy_value - entropy_value\n",
    "            if information_gain > max_info_gain:\n",
    "                max_info_gain = information_gain\n",
    "                best_feature_index = i\n",
    "                best_partition_val = best_partition_val_i\n",
    "        return best_feature_index, best_partition_val\n",
    "\n",
    "    def id3(self, X, y, original_data, features, parent_node_class=None):\n",
    "        \"\"\"\n",
    "        @description :ID3 algorithm\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @param original_data:\n",
    "        @param features:\n",
    "        @param parent_node_class:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # Firstly, it can stop at three conditions(Ignore the situation that the tree reaches a certain depth):\n",
    "        if len(np.unique(y)) <= 1:\n",
    "            # 1. If there is only one class left, return it\n",
    "            return np.unique(y)[0]\n",
    "        elif len(y) == 0:\n",
    "            # 2. If there is no data left, return the most common class\n",
    "            return np.unique(original_data[\"好瓜\"])[\n",
    "                np.argmax(np.unique(original_data[\"好瓜\"],\n",
    "                                    return_counts=True)[1])]\n",
    "        elif len(features) == 0:\n",
    "            # 3. If there is no feature left, return the parent node class（which is the most common class）\n",
    "            return parent_node_class\n",
    "        else:\n",
    "            # Then, If none of the above conditions is satisfied, continue to split\n",
    "            parent_node_class = np.unique(y)[\n",
    "                np.argmax(np.unique(y, return_counts=True)[1])]\n",
    "            best_feature = self.best_feature_to_split_id3(X, y)\n",
    "            tree = {best_feature: {}}\n",
    "            features = [i for i in features if i != best_feature]\n",
    "            for value in np.unique(X[best_feature]):\n",
    "                sub_data = X.where(X[best_feature] == value).dropna()\n",
    "                subtree = self.id3(sub_data, y[sub_data.index], original_data, features, parent_node_class)\n",
    "                tree[best_feature][value] = subtree\n",
    "            return tree\n",
    "\n",
    "    def c4_5(self, X, fes, fe_propers):\n",
    "        \"\"\"\n",
    "        @description: C4.5 algorithm\n",
    "        @param X:\n",
    "        @param fes:\n",
    "        @param fe_propers:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        # Get the class labels for each data entry\n",
    "        classList = [example[-1] for example in X]\n",
    "        # If all class labels are the same, return the class label\n",
    "        if classList.count(classList[0]) == len(classList):\n",
    "            return classList[0]\n",
    "        # If there is only one feature left, return the most common class label\n",
    "        if len(X[0]) == 1:\n",
    "            return self.most_common(classList)\n",
    "\n",
    "        # Choose the best feature and partition value for splitting\n",
    "        bestFeature, bestPartitionValue = self.best_feature_to_split_c4_5(X, fe_propers)\n",
    "\n",
    "        # If no best feature is found, return the most common class label\n",
    "        if bestFeature == -1:\n",
    "            return self.most_common(classList)\n",
    "\n",
    "        # If the best feature is discrete\n",
    "        if fe_propers[bestFeature] == 0:\n",
    "            bestFeatureLabel = fes[bestFeature]\n",
    "            Tree = {bestFeatureLabel: {}}\n",
    "            newLabels = copy.copy(fes)\n",
    "            newLabelProperties = copy.copy(fe_propers)\n",
    "            del (newLabels[bestFeature])\n",
    "            del (newLabelProperties[bestFeature])\n",
    "            featureValues = [example[bestFeature] for example in X]\n",
    "            uniqueValues = set(featureValues)\n",
    "            for value in uniqueValues:\n",
    "                subLabels = newLabels[:]\n",
    "                subLabelProperties = newLabelProperties[:]\n",
    "                Tree[bestFeatureLabel][value] = self.c4_5(self.filter_data(X, value, bestFeature), subLabels,\n",
    "                                                          subLabelProperties)\n",
    "        else:  # If the best feature is continuous\n",
    "            bestFeatureLabel = fes[bestFeature] + '<' + str(bestPartitionValue)\n",
    "            Tree = {bestFeatureLabel: {}}\n",
    "            subLabels = fes[:]\n",
    "            subLabelProperties = fe_propers[:]\n",
    "            # Build the left subtree\n",
    "            leftValue = '是'\n",
    "            Tree[bestFeatureLabel][leftValue] = self.c4_5(\n",
    "                self.filter_data_by_threshold(X, bestFeature, bestPartitionValue, 'S'), subLabels,\n",
    "                subLabelProperties)\n",
    "            # Build the right subtree\n",
    "            rightValue = '否'\n",
    "            Tree[bestFeatureLabel][rightValue] = self.c4_5(\n",
    "                self.filter_data_by_threshold(X, bestFeature, bestPartitionValue, 'T'), subLabels,\n",
    "                subLabelProperties)\n",
    "        return Tree\n",
    "\n",
    "    def post_prune(self, node, validation_data):\n",
    "        \"\"\"\n",
    "        @description: Post-pruning method for the decision tree\n",
    "        @param node: current node\n",
    "        @param validation_data: validation data set\n",
    "        @return: pruned decision tree\n",
    "        \"\"\"\n",
    "        # Check if the current node is a leaf node\n",
    "        if not isinstance(node, dict):\n",
    "            return node\n",
    "\n",
    "        # Recursively prune child nodes\n",
    "        for key in node.keys():\n",
    "            node[key] = self.post_prune(node[key], validation_data)\n",
    "\n",
    "        # Check if the current node can be pruned\n",
    "        original_accuracy = self.calculate_accuracy(validation_data['label'], self.predict(validation_data))\n",
    "        pruned_node = self.most_common(validation_data['label'])\n",
    "        pruned_accuracy = self.calculate_accuracy(validation_data['label'],\n",
    "                                                  self.predict(validation_data, pruned_node))\n",
    "\n",
    "        # If the accuracy improves after pruning, prune the node\n",
    "        if pruned_accuracy >= original_accuracy:\n",
    "            return pruned_node\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        @description: train the model based on different given methods\n",
    "        @param X:\n",
    "        @param y:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.method == 'ID3':\n",
    "            features = X.columns.tolist()\n",
    "            self.tree = self.id3(X, y, pd.concat([X, y], axis=1), features)\n",
    "            if self.puring:\n",
    "                self.tree = self.post_prune(self.tree, pd.concat([pd.DataFrame(X[:, :-1], columns=self.features), y], axis=1))\n",
    "        elif self.method == 'C4.5':\n",
    "            self.tree = self.c4_5(X, self.features, self.features_properties)\n",
    "            if self.puring:\n",
    "                self.tree = self.post_prune(self.tree, pd.concat([pd.DataFrame(X[:, :-1], columns=self.features), y], axis=1))\n",
    "        else:\n",
    "            raise Exception(\"No such method!\")\n",
    "\n",
    "    def predict(self, X, tree =None):\n",
    "        \"\"\"\n",
    "        @description: using the re-trained model to predict the result\n",
    "        @param X:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        if self.method == 'ID3':\n",
    "            # apply predict_single on every row of X\n",
    "            predictions = X.apply(self.predict_single, axis=1, args=(tree,))\n",
    "            return predictions\n",
    "        elif self.method == 'C4.5':\n",
    "            predictions = []\n",
    "            for x in X:\n",
    "                predictions.append(self.predict_single(tree, self.features, self.features_properties, x))\n",
    "        else:\n",
    "            raise Exception(\"No such method!\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def predict_single(self, DecisionTree, fes, fe_propers, test_data):\n",
    "        \"\"\"\n",
    "        @description: using the re-trained model to predict the result\n",
    "        @param DecisionTree: the whole decision tree or subtree\n",
    "        @param fes: feature labels\n",
    "        @param fe_propers: feature properties\n",
    "        @param test_data: the test data\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        if self.method == 'ID3':\n",
    "            # Traverses the decision tree on the values in 'x' until it reaches a leaf node.\n",
    "            tree = DecisionTree\n",
    "            while isinstance(tree, dict):\n",
    "                root = next(iter(tree))\n",
    "                tree = tree[root][test_data[root]]\n",
    "            # The class label of the leaf node is returned as the prediction for 'x'.\n",
    "            return tree\n",
    "        elif self.method == 'C4.5':\n",
    "            # Get the first key of the input tree\n",
    "            firstKey = list(DecisionTree.keys())[0]\n",
    "            featureName = firstKey\n",
    "            # Check if the feature is continuous\n",
    "            splitIndex = str(firstKey).find('<')\n",
    "            if splitIndex > -1:\n",
    "                featureName = str(firstKey)[:splitIndex]\n",
    "            # Get the subtree corresponding to the first key\n",
    "            subTree = DecisionTree[firstKey]\n",
    "            # Get the index of the feature in the feature labels list\n",
    "            featureIndex = fes.index(featureName)\n",
    "            predictedLabel = None\n",
    "            for key in subTree.keys():\n",
    "                if fe_propers[featureIndex] == 0:\n",
    "                    if test_data[featureIndex] == key:\n",
    "                        if type(subTree[key]).__name__ == 'dict':\n",
    "                            predictedLabel = self.predict_single(subTree[key], fes, fe_propers, test_data)\n",
    "                        else:\n",
    "                            predictedLabel = subTree[key]\n",
    "                else:\n",
    "                    partitionValue = float(str(firstKey)[splitIndex + 1:])\n",
    "                    if test_data[featureIndex] < partitionValue:\n",
    "                        if type(subTree['是']).__name__ == 'dict':\n",
    "                            predictedLabel = self.predict_single(subTree['是'], fes, fe_propers, test_data)\n",
    "                        else:\n",
    "                            predictedLabel = subTree['是']\n",
    "                    else:\n",
    "                        if type(subTree['否']).__name__ == 'dict':\n",
    "                            predictedLabel = self.predict_single(subTree['否'], fes, fe_propers, test_data)\n",
    "                        else:\n",
    "                            predictedLabel = subTree['否']\n",
    "\n",
    "            return predictedLabel\n",
    "        else:\n",
    "            raise Exception(\"No such method!\")\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        @description: calculate accuracy for true labels and predicted ones\n",
    "        @param y_true:\n",
    "        @param y_pred:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        return np.sum(y_true == y_pred) / len(y_true)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:29:09.774398200Z",
     "start_time": "2023-12-13T11:29:09.732622Z"
    }
   },
   "id": "ba5d74ff0f1a1177"
  },
  {
   "cell_type": "markdown",
   "source": [
    "然后我**重新利用剪枝的方法构造对应的ID3和C4.5决策树**，并进行预测，结果如下所示："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87526be0da3962d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ID3 Decision Tree with Puring------------------\n",
      "Classify accuracy for Decision Tree Classifier ID3 with Puring is: 75.00%\n",
      "Classify error rate for Decision Tree Classifier ID3 with Puring is: 25.00%\n",
      "------------------C4.5 Decision Tree with Puring------------------\n",
      "Classify accuracy for Decision Tree Classifier C4.5 with Puring is: 81.50%\n",
      "Classify error rate for Decision Tree Classifier C4.5 with Puring is: 18.50%\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------ID3 Decision Tree with Puring------------------\")\n",
    "# Create Decision Tree Model and train it\n",
    "Decision_Tree_Classifier_ID3_pured = DecisionTree(\"ID3\", features, feature_properties, puring=True)\n",
    "Decision_Tree_Classifier_ID3_pured.fit(X_train, y_train)\n",
    "# Predict and output the accuracy and error rate\n",
    "y_pred = Decision_Tree_Classifier_ID3_pured.predict(X_test)\n",
    "accuracy = Decision_Tree_Classifier_ID3_pured.calculate_accuracy(y_test, y_pred)\n",
    "error_rate = (1 - accuracy) * 100\n",
    "accuracy = accuracy * 100\n",
    "print(\"Classify accuracy for Decision Tree Classifier ID3 with Puring is: {:.2f}%\".format(accuracy))\n",
    "print(\"Classify error rate for Decision Tree Classifier ID3 with Puring is: {:.2f}%\".format(error_rate))\n",
    "\n",
    "\n",
    "\n",
    "print(\"------------------C4.5 Decision Tree with Puring------------------\")\n",
    "# Create Decision Tree Model and train it\n",
    "Decision_Tree_Classifier_C4_5_pured = DecisionTree(\"C4.5\", features, feature_properties, puring=True)\n",
    "Decision_Tree_Classifier_C4_5_pured.fit(train2, df_train2['好瓜'])\n",
    "# Predict and output the accuracy\n",
    "y_pred = Decision_Tree_Classifier_C4_5_pured.predict(test2)\n",
    "accuracy = Decision_Tree_Classifier_C4_5_pured.calculate_accuracy(df_test2['好瓜'], y_pred)\n",
    "error_rate = (1 - accuracy) * 100\n",
    "accuracy = accuracy * 100\n",
    "print(\"Classify accuracy for Decision Tree Classifier C4.5 with Puring is: {:.2f}%\".format(accuracy))\n",
    "print(\"Classify error rate for Decision Tree Classifier C4.5 with Puring is: {:.2f}%\".format(error_rate))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:29:56.263549100Z",
     "start_time": "2023-12-13T11:29:56.216365Z"
    }
   },
   "id": "1554a939158605df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3 结果分析\n",
    "最后结合前面初级，中级，高级的三阶段要求，让我们对比一下不同方法的决策树已经剪枝前后的分类精度和错误率。**结果如下：**\n",
    "1. **ID3决策树：**\n",
    "    - 初级要求（剪枝前）：分类精度为70%，错误率为30%\n",
    "    - 高级要求（剪枝后）：分类精度为75%，错误率为25%\n",
    "2. **C4.5决策树：**\n",
    "    - 中级要求（剪枝前）：分类精度为80%，错误率为20%\n",
    "    - 高级要求（剪枝后）：分类精度为81.5%，错误率为18.5%\n",
    "3. **ID3决策树分析**\n",
    "    - **剪枝前**：ID3决策树在小数据集上容易过拟合，因为它倾向于创建深且复杂的树，尤其是在有许多属性的情况下。这可能导致分类精度较低，因为树可能会对训练数据中的噪声和异常值过度敏感。\n",
    "    - **剪枝后**：应用后剪枝后，精度提高到了75%。这表明剪枝帮助去除了一些过拟合的部分，使树在未知数据上表现得更好。在小数据集上，剪枝可以帮助模型更好地泛化，避免对训练数据的特殊特征过度适应。\n",
    "4. **C4.5 决策树分析**\n",
    "    - **剪枝前**：C4.5决策树本身就是ID3的改进版本，它通过使用信息增益比来减少对具有多个值的属性的偏见，并能处理连续属性。这可能是为什么C4.5在剪枝前就表现出更高的分类精度（80%）。\n",
    "    - **剪枝后**：经过剪枝后C4.5的精度略微提高到了81.5%。这个小幅的提升可能是因为数据集本身不大，剪枝移除了一些不必要的分支，从而提高了模型的泛化能力。\n",
    "5. **总体分析：**\n",
    "在数据集较小时，决策树倾向于过拟合，因为它们可能会对训练数据中的每个细节都进行学习，包括噪声和异常值。后剪枝通过减少树的复杂性来提高模型的泛化能力。**在像是本次实验这种样本量较少的情况下，剪枝可能带来的结果是不稳定的。** 剪枝对于改善两种决策树的性能都是有效的，尽管对C4.5决策树的影响相对较小。这可能是因为C4.5自身就包含了一些减少过拟合的机制（如信息增益比），使得它在未剪枝的情况下也能相对较好地处理小数据集。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fdc7e564a789610"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 实验心得和收获\n",
    "\n",
    "* 本次实验中，我首先对给定的数据集进行预分析和处理，**发现了数据集量较小的情况，并对后面的可能进行了预测，果不其然，后面的实验中也出现了剪枝不稳定的情况。**\n",
    "\n",
    "* 对上课老师所讲到的决策树相关算法有了更深刻的理解。**亲自编写了ID3和C4.5决策树对我锻炼极大。**\n",
    "\n",
    "* 通过将ID3和C4.5决策树成功地**将二者封装成一个类，提供了很方便的训练和预测的接口。对我的编程能力也有了很大的提升。**\n",
    "\n",
    "* 最后在剪枝的过程中，我也**对决策树的过拟合问题有了更深刻的理解。**\n",
    "\n",
    "* 最后我还通过进一步探索和挖掘，分析使用剪枝前后ID3和C4.5决策树性能的差异。**更加加深了我对算法的理解。**\n",
    "\n",
    "总的来说我收获颇丰，也是最后一次实现的个人的机器学习作业。**在这里想要对传授我们机器学习这么底层和原理知识的卫老师表达感激，对一直帮我答疑的助教学长闫乘玮和学姐陈曦冉同样表达十分的感谢！这一学期的编程作业我收获极大，后面的大作业我也会和组员们努力完成哒~**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e15d48d460a28aa5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b48672683cee6de9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
